# ========================================
# MEDIBYTES ML SERVICE ENVIRONMENT VARIABLES
# ========================================

# -------------------- Flask Configuration --------------------
FLASK_APP=run.py
FLASK_ENV=development
FLASK_DEBUG=True

# -------------------- Server Configuration --------------------
PORT=5000
HOST=0.0.0.0

# -------------------- AI Model Configuration --------------------
# Ollama local LLM (runs on same machine as Flask)
# Download from: https://ollama.ai/download
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=llama2
# Alternative models: mistral, codellama, neural-chat

# OpenAI API (if using GPT models instead of Ollama)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your_openai_api_key_here...
OPENAI_MODEL=gpt-4-turbo-preview

# -------------------- OCR Configuration --------------------
# OCR engine: easyocr, tesseract, or both
OCR_ENGINE=easyocr

# EasyOCR language support (comma-separated)
# Supported: en, es, fr, de, hi, ar, zh, ja, ko, etc.
EASYOCR_LANGUAGES=en

# Tesseract OCR path (Windows only)
# Download from: https://github.com/UB-Mannheim/tesseract/wiki
TESSERACT_CMD=C:\Program Files\Tesseract-OCR\tesseract.exe

# OCR confidence threshold (0.0 - 1.0)
OCR_CONFIDENCE_THRESHOLD=0.6

# -------------------- Image Processing --------------------
# Maximum file size for upload (in MB)
MAX_FILE_SIZE_MB=10

# Allowed file extensions
ALLOWED_EXTENSIONS=pdf,jpg,jpeg,png,tiff,bmp

# Image preprocessing settings
IMAGE_RESIZE_MAX_WIDTH=2000
IMAGE_RESIZE_MAX_HEIGHT=2000
IMAGE_ENHANCE_CONTRAST=true
IMAGE_DENOISE=true

# -------------------- Medical Analysis Configuration --------------------
# Enable medical report analysis
ENABLE_MEDICAL_ANALYSIS=true

# Enable organ matching algorithm
ENABLE_ORGAN_MATCHING=true

# Medical terminology database path
MEDICAL_TERMS_DB=./data/medical_terms.json

# -------------------- Performance Configuration --------------------
# Use GPU acceleration (requires CUDA)
USE_GPU=false

# Number of CPU threads for processing
NUM_THREADS=4

# Maximum concurrent requests
MAX_WORKERS=4

# Request timeout (seconds)
REQUEST_TIMEOUT=120

# -------------------- Caching Configuration --------------------
# Enable result caching
ENABLE_CACHE=true

# Cache TTL in seconds (3600 = 1 hour)
CACHE_TTL=3600

# Cache backend: simple, redis
CACHE_TYPE=simple

# Redis configuration (if using Redis)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# -------------------- CORS Configuration --------------------
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://localhost:8000,http://127.0.0.1:3000,http://127.0.0.1:8000

# -------------------- Logging Configuration --------------------
LOG_LEVEL=INFO
LOG_FILE=./logs/ml_service.log
LOG_REQUESTS=true

# -------------------- Security Configuration --------------------
# API key for authentication (optional)
# Generate with: openssl rand -hex 32
ML_API_KEY=your_random_api_key_here_if_needed

# Rate limiting (requests per minute)
RATE_LIMIT=60

# -------------------- Storage Configuration --------------------
# Upload directory for temporary files
UPLOAD_DIR=./uploads

# Auto-cleanup uploads after processing
AUTO_CLEANUP=true

# Cleanup delay in seconds (300 = 5 minutes)
CLEANUP_DELAY=300

# Model storage path
MODEL_PATH=./ml_models

# -------------------- Health Check Configuration --------------------
ENABLE_HEALTH_CHECK=true
HEALTH_CHECK_PATH=/health

# ========================================
# SETUP INSTRUCTIONS:
# ========================================
# 1. Copy this file: cp .env.example .env
# 2. Install Python dependencies: pip install -r requirements.txt
# 3. Download Ollama: https://ollama.ai/download
# 4. Pull model: ollama pull llama2
# 5. Install Tesseract (Windows):
#    https://github.com/UB-Mannheim/tesseract/wiki
# 6. Run service: python run.py
# ========================================

# ========================================
# USEFUL COMMANDS:
# ========================================
# Start ML service:
#   python run.py
#
# Start with gunicorn (production):
#   gunicorn -w 4 -b 0.0.0.0:5000 run:app
#
# Test OCR endpoint:
#   curl -X POST http://localhost:5000/api/ocr/extract \
#     -F "file=@test_report.pdf"
#
# Test analysis endpoint:
#   curl -X POST http://localhost:5000/api/analysis/simple \
#     -H "Content-Type: application/json" \
#     -d '{"text": "Patient has diabetes type 2"}'
#
# Health check:
#   curl http://localhost:5000/health
# ========================================

# ========================================
# DOCKER DEPLOYMENT:
# ========================================
# Build image:
#   docker build -t medibytes-ml .
#
# Run container:
#   docker run -p 5000:5000 --env-file .env medibytes-ml
#
# Docker Compose:
#   docker-compose up ml-service
# ========================================